# SSD: Single Shot Detector

### Object Detection Models
<img src="https://user-images.githubusercontent.com/33839093/108666988-c84d2d80-751b-11eb-89a0-5b4ac7917679.png">

> 2-stage detector: region proposal(RPN)처럼 bbox를 만드는 네트워크가 따로 존재하는 모델
> 1-stage detector: region도 뽑으면서 classification도 한번에(하나의 네트워크에서) 하는 모델

Faster R-CNN : 성능은 좋지만, 속도가 너무 느림, 실시간으로 object detection을 할 수 없음(7 FPS with mAP 73.2%)
YOLO : 속도는 빠르지만 성능이 좋지 못함(45 FPS with mAP 63.4%)
SSD : 속도와 성능 두마리 토끼를 모두 잡겠다!!(59 FPS with mAP 74.3% on VOC2007)
(YOLO는 독자적인 구조를 가지며, 1-stage detector는 대부분 SSD를 기반으로 함)

### YOLO의 단점
- 입력 이미지를 7*7 크기의 그리드로 나누어 각 그리드별로 bbox prediction을 진행해 그리드 크기보다 작은 물체는 찾기 어려움
- 컨볼루션과 풀링을 거친 마지막 feature map하나만 사용하기 때문에 정확도가 하락함

### Multi-scale Feature Maps for detection
<img src="https://user-images.githubusercontent.com/33839093/108667413-81136c80-751c-11eb-84cf-433c1bdbf7d3.png">

-> SSD는 YOLO와 달리 컨볼루션을 거치는 과정의 중간 중간에 있는 feature map에서도 object detection을 수행한다.

> 300x300크기의 이미지를 입력받아 imagenet으로 pre-trained된 VGG의 conv5_3까지 통과해 feature를 추출함.
> 추출한 feature를 다음 층에 넘겨주면서, object detection을 수행함.
> 최종적으로는 1x1크기의 feature map까지 뽑음.

<img src="https://user-images.githubusercontent.com/33839093/108667554-d3ed2400-751c-11eb-8827-99b2dac3227f.png">
그림의 detector&classifier에서 object detection을 수행하며, 총 6번의 object detection을 수행함

### Detector & classifier
<img src="https://user-images.githubusercontent.com/33839093/108667556-d5b6e780-751c-11eb-852d-37cfce154619.png">
컨볼루션 중간에 5x5x256크기의 feature map을 대상으로 object detection을 수행한다고 가정. (5x5는 YOLO에서 그리드 크기라고 생각)

하나의 그리드마다 크기가 다른 default box를 계산: default box는 Faster R-CNN에서 anchor box처럼 bbox를 찾는데 도움을 주는 장치? (YOLO는 기본 값 없이 bbox를 예측하게 함)

### Default boxes and Aspect ratio : 각각의 feature map마다 몇 개의 bbox를 뽑을까?

<img src="https://user-images.githubusercontent.com/33839093/108668014-afde1280-751d-11eb-9a68-a4493adef1de.png">
19x19x1024 크기의 feature map에서 하나의 bbox를 1개 뽑으려면 (c+4)xk개의 filter가 필요함.

> 4 : x, y, w, h
> c : 클래스의 개수, 논문에서 사용한 pascal VOC 데이터셋은 클래스가 20개이며, 배경을 추가해서 21개의 클래스로 만듦
> k : 하나의 location에서 뽑는 bbox의 개수, 이 경우에는 6
> output channel = 6x(21+4) = 150
> 150 : 1번째 bbox의 xywh 4개+1번째 bbox의 class score 21개+2번째…+6번째 bbox의 xywh 4개+6번째 bbox의 class score 21개

<img src="https://user-images.githubusercontent.com/33839093/108668017-b10f3f80-751d-11eb-8c0e-361f09958d75.png">

> 큰 물체(강아지)는 뒤쪽 feature map에서 detect
> 작은 물체(고양이)는 앞쪽 feature map에서 detect
> 빨간 박스(강아지)는 8*8에서 iou값이 0.5보다 큰 경우를 찾을 수 없기 때문
> 👉 SSD는 multiple feature map을 사용해 앞쪽의 해상도가 높은 feature map에서는 작은 물체를 detect하고, 뒤쪽의 해상도가 낮은 feature map에서는 큰 물체를 detect함

### Default Boxes Generating
<img src="https://user-images.githubusercontent.com/33839093/108668305-50343700-751e-11eb-921a-e0b60a519740.png" width="500">

> m : feature map의 개수=6  
> s_min=0.2, s_max=0.9이며 min과 max값을 정한 후 그 사이를 m값에 따라 구간을 나누어줌
-> m=6으로 설정하면 [0.2, 0.34, 0.48, 0.62, 0.76, 0.9]가 된다
-> 첫번째 feature map에서는 입력 이미지의 0.2비율인 박스를 default box로 설정하겠다는 의미

<img src="https://user-images.githubusercontent.com/33839093/108668311-51fdfa80-751e-11eb-9dda-0de4b739569b.png" width="270">

> 정사각형 뿐만 아니라 다양한 비율(a)을 가진 default box를 구함
> {1, 2, 3, 1/2, 1/3}중 하나의 값을 가지는 a는 aspect ratio로, 위에서 정한 s_k값과 a값에 따라 default box의 너비, 높이를 계산

### Loss Function
<img src="https://user-images.githubusercontent.com/33839093/108668793-2c252580-751f-11eb-9bbe-7ba1d8973318.png" width="500">

N = bbox중 매치된 개수

전체 loss = 각 클래스별로 예측한 값인 L_conf + bbox regression값과 실제값의 차인 L_loc

<img src="https://user-images.githubusercontent.com/33839093/108668795-2cbdbc00-751f-11eb-9c25-279327367a25.png" width="500">

> L_conf : cross-entropy 사용

<img src="https://user-images.githubusercontent.com/33839093/108668797-2d565280-751f-11eb-88fd-ad8cb063165a.png" width="500">

> L_loc: smooth L1 loss 사용

### Hard Negative Mining
- SSD가 뽑은 8700개의 bbox중, 3개의 물체에 대해 각각 10개씩 bbox가 쳐지면, 나머지 8400개는 배경에 대한 bbox임.
- 배경에 대한 bbox가 너무 많아 데이터가 imbalance함-> hard negative mining으로 해결함
- Confidence loss가 큰 순서대로 정렬시킨 후, bbox를 뽑아서 사용함(배경인데 배경으로 판단하지 않은 것들을 사용함)
positive:negative 이미지 비율=1:3으로 학습에 사용함

### SSD의 단점
- 첫번째 feature map에서 작은 물체를 detect하는데, 첫번째 feature map이 충분히 abstract하지 않아서 작은 물체를 잘 찾지 못함-> RetinaNet에서는 작은 물체를 잘 찾기 위해 올라갔다가 내려오는? 구조를 사용함
- SSD에서는 data augmentation을 통해 이 문제를 극복함(작은 물체가 있는 데이터를 만듦)

### References
[갈아먹는 머신러닝](https://yeomko.tistory.com/20)
[PR-132: SSD: Single Shot MultiBox Detector](https://www.youtube.com/watch?v=ej1ISEoAK5g)
